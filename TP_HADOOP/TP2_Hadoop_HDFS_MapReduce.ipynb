{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfS8OXBsYKlH"
   },
   "source": [
    "# Initiation à Hadoop et Map-Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsG46IqwYKlO"
   },
   "source": [
    "L'Objectif du TP: \n",
    "Initiation au framework hadoop et au patron MapReduce, utilisation de docker pour lancer un cluster hadoop.\n",
    "Les consignes de ce notebook sont à réaliser dans votre cluster deployé avec docker, mais vous pouvez utiliser le notebook pour stocker les commandes que vous avez utilisé afin de réaliser une sorte de cheat sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yd2lb_CQYKlP"
   },
   "source": [
    "- Clonez le repo à l’adresse https://github.com/big-data-europe/docker-hadoop dans votre répertoire de travail.\n",
    "\n",
    "\n",
    "- Démarrez votre image docker: \\\n",
    "Ouvrez votre terminal et placez-vous dans le dossier docker-hadoop fraîchement récupéré. \\\n",
    "Lancez la commande: \\\n",
    "\tdocker-compose up -d\n",
    "\n",
    "\n",
    "- Connectez vous au namenode de votre cluster:\\\n",
    "Lancez la commande: \\\n",
    "    docker exec -it namenode bash\n",
    "\n",
    "\n",
    "Refs: \\\n",
    "Apache Hadoop [http://hadoop.apache.org/] \\\n",
    "Docker [https://www.docker.com/]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5GFRNaNYKlQ"
   },
   "source": [
    "- Placez vous dans le dossier utilisateur: \\\n",
    "cd ~\n",
    "\n",
    "\n",
    "- Créer un répertoire TP, puis deux sous-répertoires code et data dans lesquels sauvegarder respectivement les codes des mappers et reducers, et les données sources et résultats.\n",
    "\n",
    "\n",
    "- En local (en dehors de docker), avec la commande docker adequate envoyez le fichier ventes.txt fourni, dans le répertoire ~/TP/data du namenode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwGdKcZKYKlQ"
   },
   "source": [
    "### 1- Hadoop HDFS\n",
    "   \n",
    "##### Les commandes les plus utilisées dans Hadoop: \n",
    "hdfs dfs –ls             Afficher le contenu du répertoire racine\n",
    "\n",
    "hdfs dfs –put file.txt   Upload un fichier dans hadoop (à partir du répertoire courant linux)\n",
    "\n",
    "hdfs dfs –get file.txt   Download un fichier à partir de hadoop sur votre disque local\n",
    "\n",
    "hdfs dfs –tail file.txt  Lire les dernières lignes du fichier\n",
    "\n",
    "hdfs dfs –cat file.txt   Affiche tout le contenu du fichier\n",
    "\n",
    "hdfs dfs –mv file.txt newfile.txt   Renommer le fichier\n",
    "\n",
    "hdfs dfs –rm newfile.txt            Supprimer le fichier\n",
    "\n",
    "hdfs dfs –mkdir myinput             Créer un répertoire\n",
    "\n",
    "hdfs dfs –cat file.txt | less       Lire le fichier page par page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wW_IumP7YKlQ"
   },
   "source": [
    "- Créez un répertoire dans HDFS, appelé data.\n",
    "\n",
    "\n",
    "- Envoyez le fichier ventes.txt depuis le dossier ~/TP/data vers le répertoire data HDFS.\n",
    "\n",
    "\n",
    "- Affichez le contenu du répertoire data HDFS et visualisez les dernières lignes du fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnvL2p6AYKlR"
   },
   "outputs": [],
   "source": [
    "Dans Windows terminal:\n",
    "docker ps\n",
    "docker cp ventes.txt 5ba:/TP/data/\n",
    "\n",
    "Dans docker terminal:\n",
    "/ls\n",
    "mkdir /TP\n",
    "cd TP/\n",
    "mkdir data\n",
    "cd data/\n",
    "pwd\n",
    "ls\n",
    "hdfs dfs -put ventes.txt ./TP/data\n",
    "\n",
    "cat /TP/data/ventes.txt | /TP/code/mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFLivgK0YKlR"
   },
   "source": [
    "### 2-  Map Reduce\n",
    "Un Job Map-Reduce se compose principalement de deux types de programmes:\n",
    "\n",
    "Mappers : permettent d’extraire les données nécessaires sous forme de clef/valeur, pour pouvoir ensuite appliquer le shuffle &sort. \\\n",
    "Reducers : prennent un ensemble de données triées , et effectuent le traitement nécessaire sur ces données (somme, moyenne,total...)\n",
    "\n",
    "PS : Vous aurez besoin d'installer python dans votre namenode docker avec apt-get pour tester vos mapper et reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Install Python:\n",
    "    apt-get update\n",
    "    apt-get install python3\n",
    "    apt-get upgrade\n",
    "    \n",
    "    apt install nano\n",
    "    \n",
    "    ls -hal # voir les droits pour les fichiers\n",
    "    chmod +rwx mapper.py # ajouter les droit pour ecrire, executer et lire\n",
    "    nano mapper.py # pour modifier le fichier\n",
    "    \n",
    "    \n",
    "https://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUVNEI6JYKlS"
   },
   "source": [
    "##### 2.1 )\n",
    "\n",
    "Soit le dataset ventes.txt comportant 6 champs, séparés par des tabulations, de la forme suivante:\n",
    "\n",
    "date temps magasin produit coût paiement\n",
    "\n",
    "Le but est de déterminer le total des ventes par magasin.\n",
    "\n",
    "- Créez un fichier mapper.py dans le dossier ~/TP/code , qui permet de: \\\n",
    "Séparer les différents champs par tabulation. \\\n",
    "Extraire les éléments à partir de ces champs, sous forme de clé/valeur (magasin,coût), Pour calculer les ventes par magasin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMKxkzlzYKlS"
   },
   "outputs": [],
   "source": [
    "#mapper.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "        date, temps, magasin, produit, cout, paiement = line.strip().split('\\t')\n",
    "        print('{0}\\t{1}'.format(magasin, cout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eELMamjdYKlS"
   },
   "source": [
    "- Testez votre mapper sur les 50 premières lignes du fichier ventes.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I74Ui2u_YKlS"
   },
   "outputs": [],
   "source": [
    "cat /TP/data/ventes.txt | /TP/code/mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LKGY0AnYKlT"
   },
   "source": [
    "##### 2.2 )\n",
    "Le Reducer permet de faire le traitement définit sur des entrées préalablement triées par Hadoop sur les couples (magasin,coût). \\\n",
    "Le Reducer fera la somme de tous les coûts pour un même magasin. \n",
    "\n",
    "- Créez un fichier reducer.py dans le dossier ~/TP/code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmXuhyuwYKlT"
   },
   "outputs": [],
   "source": [
    "#reducer.py\n",
    "#!/usr/bin/python3\n",
    "import sys\n",
    "\n",
    "\n",
    "ventes_dict = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "        magasin, cout = line.strip().split('\\t')\n",
    "        if magasin not in ventes_dict:\n",
    "                ventes_dict[magasin] = float(cout)\n",
    "        else:\n",
    "                ventes_dict[magasin] += float(cout)\n",
    "\n",
    "for key, value in ventes_dict.items():\n",
    "        print('{0}\\t{1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6zn_AXZYKlU"
   },
   "source": [
    "- Testez votre Reducer sur les 50 premières lignes du fichier ventes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRK5-47QYKlU"
   },
   "outputs": [],
   "source": [
    "cat /TP/data/ventes.txt | /TP/code/mapper.py | /TP/code/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrZTldnqYKlU"
   },
   "source": [
    "##### 2.3)\n",
    "Lancer un job entier sur Hadoop =>  faire appel au mapper puis au reducer sur une entrée volumineuse, et obtenir à la fin un résultat, directement sur HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVV1XweMYKlV"
   },
   "source": [
    "- Exécutez un job hadoop sur le fichier ventes.txt en utilisant les fichiers mapper.py et reducer.py déjà implémentés. \n",
    "- Stocker le résultat dans un répertoire joboutput.\n",
    "\n",
    "\n",
    "- Sauvegarder ensuite le fichier part-00000 dans le dossier ~/TP/data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8C9x0PU2YKlV"
   },
   "outputs": [],
   "source": [
    "hdfs dfs -mkdir -p TP\n",
    "hdfs dfs -ls\n",
    "hdfs dfs -mkdir -p TP/code\n",
    "hdfs dfs -mkdir -p TP/data\n",
    "hdfs dfs -ls TP\n",
    "\n",
    "hdfs dfs -put /TP/code/mapper.py ./TP/code\n",
    "hdfs dfs -put /TP/code/reducer.py ./TP/code\n",
    "\n",
    "hdfs dfs -ls TP/code\n",
    "hdfs dfs -chmod +rwx TP/code/mapper.py\n",
    "hdfs dfs -chmod +rwx TP/code/reducer.py\n",
    "\n",
    "hdfs dfs -cat ./TP/data/ventes.txt | /TP/code/mapper.py | /TP/code/reducer.py #faux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install python in all images\n",
    "docker exec -it namenode bash\n",
    "apt update && apt upgrade -y && apt install python3 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92sXnLNMYKlV"
   },
   "outputs": [],
   "source": [
    "/opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar\n",
    "\n",
    "hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar -file ./mapper.py -file ./reducer.py -mapper mapper.py -reducer reducer.py -input ./TP/data/ventes.txt -output output-data1\n",
    "\n",
    "hadoop fs -cat ./output-data1/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJyHJrfSYKlW"
   },
   "source": [
    "- Quelle est la totalité des ventes du magasin de Buffalo ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89gqLHtPYKlW"
   },
   "outputs": [],
   "source": [
    "483.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpYdenfcYKlW"
   },
   "source": [
    "- Créez un nouveau mapper/reducer pour trouver le total des ventes par catégorie de produits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZqA3F-PYKlW"
   },
   "outputs": [],
   "source": [
    "#mapper2.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PZiAT3YYKlW"
   },
   "outputs": [],
   "source": [
    "#Le reducer NE CHANGE PAS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXIRquxdYKlX"
   },
   "source": [
    "- Exécutez un job hadoop sur le fichier ventes.txt en utilisant les fichiers mapper2.py et reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0QPZDFpYKlX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofvlL97tYKlX"
   },
   "source": [
    "- Quelle est la valeur des ventes pour la catégorie Toys?\n",
    "\n",
    "\n",
    "- Et pour la catégorie Consumer Electronics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oU68kXQ4YKlX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Takze-DiYKlX"
   },
   "source": [
    "##### Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkOK_7T0YKlY"
   },
   "source": [
    "- Créez un nouveau mapper/reducer pour faire la liste des montants de ventes pour chaque magasin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8aPY0FHYKlY"
   },
   "outputs": [],
   "source": [
    "#mapper3.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAGi46eiYKlY"
   },
   "outputs": [],
   "source": [
    "#Reducer2.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4aCZa2mYKlY"
   },
   "source": [
    "- Quelle est la valeur de la vente la plus élévée pour les magasins suivants:\n",
    "\n",
    "o Reno\n",
    "\n",
    "o Toledo\n",
    "\n",
    "o Chandler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_Tgx4vwYKlY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1bjTjEbYKlY"
   },
   "source": [
    "- Quel est le nombre total des ventes et la valeur totale des ventes de tous magasins confondus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbXrMrAjYKlZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TP2_Hadoop_HDFS_MapReduce.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
